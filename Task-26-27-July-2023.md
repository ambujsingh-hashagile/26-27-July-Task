# Task Date: 26th & 27th/July/2023

Task: Using Python Load the data from https://api.patentsview.org/patents/query?q=%7B%22_gte%22:%7B%22patent_date%22:%222007-01-04%22%7D%7D&f=%5B%22patent_number%22,%22patent_date%22,%22patent_title%22%5D&o=%7B%22page%22:1,%22per_page%22:25%7D to Postgres Table
 
Loop page by page and load 1000 rows. Code is already someone else. Use that
 
* Create a fast API which takes input param of limit
    * in the fast API, fetch  rows from table with limit = p_limit
    * and return the json data
Using Python
* in a infinite while loop
* add a delay of random(1, 30) seconds
* Call the Fast API by passing p_limit = random(1,10)
* post the data to new Kafka Topic
Using Spark Streaming
* Read the data from the topic
* wait for 5 rows and create a json array (Shanthi knows what I am speaking about)
* Create a Elastic Search Collection
* Push the batch data to ES collection from Spark Streaming

——————————————————————————————————————>

Steps to follow:
- [x] Create a docker-compose.yaml
- [x] Using python load data from API to Postgres table
- [x] Loop page by page and load 1000 rows 
- [x] Create a fastAPI which takes input param of limit and return that no. of rows data in JSON
- [x] In an infinite while loop create a delay of rand(1,30) seconds
- [x] Call fastAPI by passing limit = rand(1,10)
- [x] Post the data to new Kafka topic
Use spark streaming. (Didn’t work tried Structured Streaming but that also didn’t work)
- [x] Read data from the topic 
- [x] Wait for the 5 rows and create a json array
- [x] Create an elastic search collection
- [x] Push the Batch data to es collection from the spark streaming
Further addition in the task
- [x] Push Kafka messages to solr collection also
——————————————————————————————————————>

## Working Code for the given task :-

#### See elastic search output using pretty : curl -X GET "http://localhost:9200/new_patents/_search" | jq


### File: docker-compose-debezium.yaml (you can get this code from that Kafka debezium docker tutorial guy on yt)

```version: "3.3"
services:
  postgres:
    image: debezium/example-postgres:1.9
    container_name: postgres
    ports:
      - 5433:5432
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres

  # 1. kafka & zookeeper --> docker
  # 2. kafdrop --> docker
  # 3. postgres --> docker
  # 4. debezium connector --> docker
  # 5. pgAdmin/DBeaver --> local system
  # 6. Postman --> local system

  # Zookeeper, single node
  zookeeper:
    image: wurstmeister/zookeeper:latest
    platform: linux/amd64 # Specify the platform explicitly
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - 2181:2181
      - 2888:2888
      - 3888:3888

  # kafka multi node
  kafka1:
    image: wurstmeister/kafka:latest
    platform: linux/arm64 # Specify the platform explicitly
    restart: "no"
    links:
      - zookeeper
    ports:
      - 9092:9092
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: INTERNAL://:29092,EXTERNAL://:9092
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka1:29092,EXTERNAL://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      #https://github.com/wurstmeister/kafka-docker/issues/553

  kafka2:
    image: wurstmeister/kafka:latest
    restart: "no"
    links:
      - zookeeper
    ports:
      - 9093:9093
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: INTERNAL://:29093,EXTERNAL://:9093
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka2:29093,EXTERNAL://localhost:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      #https://github.com/wurstmeister/kafka-docker/issues/553

  #kafdrop for topic/msg visualization
  kafdrop:
    image: obsidiandynamics/kafdrop
    platform: linux/amd64
    restart: "no"
    environment:
      KAFKA_BROKERCONNECT: "kafka1:29092,kafka2:29093"
      JVM_OPTS: "-Xms16M -Xmx512M -Xss180K -XX:-TieredCompilation -XX:+UseStringDeduplication -noverify"
    ports:
      - 9000:9000
    depends_on:
      - kafka1
      - kafka2

  # debezium connector
  kconnect:
    image: debezium/connect:1.9
    ports:
      - 8083:8083
    environment:
      CONFIG_STORAGE_TOPIC: my_connect_configs
      OFFSET_STORAGE_TOPIC: my_connect_offsets
      STATUS_STORAGE_TOPIC: my_connect_statuses
      BOOTSTRAP_SERVERS: kafka1:29092,kafka2:29093
    links:
      - zookeeper
      - postgres
    depends_on:
      - kafka1
      - kafka2
      - zookeeper
      - postgres
# POST  http://localhost:8083/connectors --> To register the kafka connector
# {
#   "name": "inventory-connector",
#   "config": {
#     "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
#     "database.hostname": "postgres",
#     "database.port": "5432",
#     "database.user": "postgres",
#     "database.password": "postgres",
#     "database.dbname" : "postgres",
#     "database.server.name": "dbserver1",
#     "table.include.list": "inventory.customers"

```

### File : kafka_producer_to_fetch_data_to_the_topic.py
```
import requests
import json
import time
from random import randint
from kafka import KafkaProducer

url = "http://127.0.0.1:8000/fetch_data"

bootstrap_servers = "localhost:9092"
topic_name = "patent_data_topic"

producer = KafkaProducer(bootstrap_servers=bootstrap_servers,
                         value_serializer=lambda v: json.dumps(v).encode("utf-8"))

while True:
    limit = randint(1, 10)
    # limit = randint(1,1000)

    response = requests.get(f"{url}?limit={limit}")
    data = response.json()

    # Send each patent as a separate message to the Kafka topic
    for patent in data:
        producer.send(topic_name, value=patent)

    print("Data Posted to Kafka Topic:")
    print(data, "\n\n\n")

    delay = randint(1, 30)
    # delay = 1
    time.sleep(delay)



```
### File: fastAPI.py
```
from fastapi import FastAPI, HTTPException
import psycopg2
from typing import List

# Database connection parameters
db_host = 'localhost'
db_port = 5433
db_name = 'postgres'
db_user = 'postgres'
db_password = 'postgres'

app = FastAPI()


def get_patent_data(limit: int) -> List[dict]:
    try:
        conn = psycopg2.connect(
            host=db_host,
            port=db_port,
            dbname=db_name,
            user=db_user,
            password=db_password
        )
        cur = conn.cursor()

        query = f"SELECT * FROM patent_data_table LIMIT {limit};"
        cur.execute(query)

        columns = [desc[0] for desc in cur.description]
        result = [dict(zip(columns, row)) for row in cur.fetchall()]

        cur.close()
        conn.close()
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/fetch_data/")
async def fetch_data(limit:service elasticsearch start int):
    if limit <= 0:
        raise HTTPException(
            status_code=400, detail="Limit must be a positive integer.")

    data = get_patent_data(limit)
    return data

```

### Load data from this api:https://api.patentsview.org/patents/query?q=%7B%22_gte%22:%7B%22patent_date%22:%222007-01-04%22%7D%7D&f=%5B%22patent_number%22,%22patent_date%22,%22patent_title%22%5D&o=%7B%22page%22:1,%22per_page%22:25%7D  to Postgres file (python code)

```
import psycopg2
import requests
import time

# Database connection parameters
db_host = 'localhost'
db_port = 5433
db_name = 'postgres'
db_user = 'postgres'
db_password = 'postgres'

# Function to create a new connection and cursor
def create_connection():
    return psycopg2.connect(
        host=db_host,
        port=db_port,
        dbname=db_name,
        user=db_user,
        password=db_password
    )

def fetch_data_from_api(url, page):
    params = {'page': page, 'per_page': 25}
    response = requests.get(url, params=params)
    if response.status_code == 200:
        data = response.json()
        return data.get('patents', [])
    else:
        print(f"Failed to fetch data from API. Status code: {response.status_code}")
        return []

def insert_data_into_table(cursor, data):
    insert_query = "INSERT INTO patent_data_table (patent_number, patent_date, patent_title) VALUES (%s, %s, %s);"
    cursor.executemany(insert_query, data)

if __name__ == "__main__":
    url = 'https://api.patentsview.org/patents/query?q={"_gte":{"patent_date":"2007-01-04"}}&f=["patent_number","patent_date","patent_title"]'
    table_name = 'patent_data_table'

    with create_connection() as conn, conn.cursor() as cursor:
        for page in range(1, 41):  # Assuming there are 40 pages (1000 rows per page, total 1000*40 = 40000 rows)
            data = fetch_data_from_api(url, page)
            if not data:
                break

            valid_data = [(item['patent_number'], item['patent_date'], item['patent_title']) for item in data]
            insert_data_into_table(cursor, valid_data)
            conn.commit()
            print(page, f"Inserted {len(valid_data)} rows into {table_name}")

            time.sleep(5)

    print("Data loading completed.")


```



### File: Kafka_consumer_stream_to_elasticsearch.py

```
from kafka import KafkaConsumer
import json
import random
from elasticsearch import Elasticsearch

bootstrap_servers = "localhost:9092"
topic_name = "patent_data_topic"
group_id = "Combiner Lag"

# Elasticsearch connection
es = Elasticsearch(["http://localhost:9200"])

# Index settings for the Elasticsearch index
mappings = {
    "properties": {
        "patent_number": {"type": "keyword"},
        "patent_date": {"type": "date"},
        "patent_title": {"type": "text"}
    }
}

# Create the Elasticsearch index with the specified mappings
if not es.indices.exists(index="new_patents"):
    es.indices.create(index="new_patents", body={"settings": {"number_of_shards": 1, "number_of_replicas": 1}, "mappings": mappings})

data_array = []

def process_data():
    global data_array
    print("\n\n\nReceived Messages:")
    for data in data_array:
        print(data,"\n\n\n")
        try:
            # Indexing data to Elasticsearch
            for i, row in enumerate(data):
                doc = {
                    "patent_number": row["patent_number"],
                    "patent_date": row["patent_date"],
                    "patent_title": row["patent_title"]
                }
                
            
                es.index(index="new_patents", id=i, body=doc)
        except Exception as e:
            print("Error indexing data:", e)
    data_array = []

try:
    consumer = KafkaConsumer(topic_name,
                             group_id=group_id,
                             bootstrap_servers=bootstrap_servers,
                             value_deserializer=lambda x: json.loads(x.decode('utf-8')))

    for message in consumer:
        data = message.value
        data_array.append(data)
        if len(data_array) >= 5:
            process_data()
            print(data_array)
            data_array = []

except KeyboardInterrupt:
    pass
except Exception as e:
    print("Error:", e)
```

### File: docker-compose.yaml (for elastic search and kibana)

```
version: "3.7"
services:
  es01:
    image: "docker.elastic.co/elasticsearch/elasticsearch-oss:7.10.2"
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      node.name: es01
      discovery.seed_hosts: es01,es02,es03
      cluster.initial_master_nodes: es01,es02,es03
      cluster.name: mycluster
      bootstrap.memory_lock: "true"
      ES_JAVA_OPTS: -Xms256m -Xmx256m
    volumes:
      - "es-data-es01:/usr/share/elasticsearch/data"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    healthcheck:
      test: ["CMD-SHELL", "curl http://localhost:9200"]
      interval: 10s
      timeout: 10s
      retries: 120
  es02:
    image: "docker.elastic.co/elasticsearch/elasticsearch-oss:7.10.2"
    ports:
      - "9201:9200"
      - "9301:9300"
    environment:
      node.name: es02
      discovery.seed_hosts: es01,es02,es03
      cluster.initial_master_nodes: es01,es02,es03
      cluster.name: mycluster
      bootstrap.memory_lock: "true"
      ES_JAVA_OPTS: -Xms256m -Xmx256m
    volumes:
      - "es-data-es02:/usr/share/elasticsearch/data"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    healthcheck:
      test: ["CMD-SHELL", "curl http://localhost:9200"]
      interval: 10s
      timeout: 10s
      retries: 120
  es03:
    image: "docker.elastic.co/elasticsearch/elasticsearch-oss:7.10.2"
    ports:
      - "9202:9200"
      - "9303:9300"
    environment:
      node.name: es03
      discovery.seed_hosts: es01,es02,es03
      cluster.initial_master_nodes: es01,es02,es03
      cluster.name: mycluster
      bootstrap.memory_lock: "true"
      ES_JAVA_OPTS: -Xms256m -Xmx256m
    volumes:
      - "es-data-es03:/usr/share/elasticsearch/data"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    healthcheck:
      test: ["CMD-SHELL", "curl http://localhost:9200"]
      interval: 10s
      timeout: 10s
      retries: 120
  kibana:
    image: docker.elastic.co/kibana/kibana-oss:7.10.2
    depends_on:
      es01:
        condition: service_healthy
      es02:
        condition: service_healthy
      es03:
        condition: service_healthy
    ports:
      - "5601:5601"
    environment:
      - 'ELASTICSEARCH_HOSTS=["http://es01:9200","http://es02:9200","http://es03:9200"]'
volumes:
  es-data-es01:
  es-data-es02:
  es-data-es03:

  ```


### Working code of kafka_consumer_to_fetch_elasticsearch.py ( just before solr included in this )
```
from kafka import KafkaConsumer
import json
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk
import threading
from queue import Queue

bootstrap_servers = "localhost:9092"
topic_name = "patent_data_topic"
group_id = "Combiner Lag"

es = Elasticsearch(["http://localhost:9200"])

mappings = {
    "properties": {
        "patent_number": {"type": "keyword"},
        "patent_date": {"type": "date"},
        "patent_title": {"type": "text"}
    }
}

if not es.indices.exists(index="new_patents_v1"):
    es.indices.create(index="new_patents_v1", body={"settings": {
        "number_of_shards": 1, "number_of_replicas": 1}, "mappings": mappings})

data_queue = Queue()
data_lock = threading.Lock()


def process_data():
    global data_queue
    if data_queue.empty():
        return

    print("\n\n\nReceived Messages:")
    try:
        bulk_data = []
        while not data_queue.empty():
            patent = data_queue.get()
            doc = {
                "_index": "new_patents_v1",
                "_id": patent["patent_number"],
                "_source": {
                    "patent_number": patent["patent_number"],
                    "patent_date": patent["patent_date"],
                    "patent_title": patent["patent_title"]
                }
            }
            bulk_data.append(doc)

        #response = bulk(es, bulk_data, index="new_patents_v1")

        print("Batch indexing done")
    except Exception as e:
        print("Error indexing data:", e)


def consume_data():
    try:
        consumer = KafkaConsumer(topic_name,
                                 group_id=group_id,
                                 bootstrap_servers=bootstrap_servers,
                                 value_deserializer=lambda x: json.loads(x.decode('utf-8')))

        for message in consumer:
            data = message.value
            with data_lock:
                if isinstance(data, list):
                    # If the received data is a list, add each patent to the queue separately
                    for patent in data:
                        data_queue.put(patent)
                else:
                    # If the received data is not a list, it is a single patent, add it to the queue
                    data_queue.put(data)

            if data_queue.qsize() >= 5:
                process_data()

    except KeyboardInterrupt:
        pass
    except Exception as e:
        print("Error:", e)
    finally:
        # Process any remaining data in the queue before exiting
        process_data()


# Start consuming data from Kafka
consume_data()

```

### kafka_consumer_solr.py

```

import json
import pysolr
from kafka import KafkaConsumer
from datetime import datetime

bootstrap_servers = "localhost:9092"
topic_name = "patent_data_topic"
group_id = "Combiner Lag"

now = datetime.now()
current_time = now.strftime("%H:%M:%S")

solr_url = 'http://localhost:8983/solr/patents'

def index_data_to_solr(data):
    solr = pysolr.Solr(solr_url, timeout=10)
    solr.add(data)
    solr.commit()

# Implement a Kafka consumer that reads data from the Kafka topic and calls the indexing function
def consumer_data():
    consumer = KafkaConsumer(topic_name,
                             group_id=group_id,
                             bootstrap_servers=bootstrap_servers,
                             value_deserializer=lambda x: json.loads(x.decode('utf-8')))

    for message in consumer:
        print("\n\n\n\n\n", message)  # testing if message received

        index_data_to_solr([message.value])

        # Print the message to confirm it was indexed
        print("Data indexed to Solr:", message.value)

if __name__ == "__main__":
    consumer_data()
```


### Posted data in solr collection (only 5 rows shown here)
```
http://localhost:8983/solr/patents/select?indent=true&q.op=OR&q=*%3A*&rows=5&useParams=
{
"responseHeader":{
"status":0,
"QTime":8,
"params":{
  "q":"*:*",
  "indent":"true",
  "q.op":"OR",
  "rows":"5",
  "useParams":"",
  "_":"1690526246766"
}
},
"response":{
"numFound":388,
"start":0,
"numFoundExact":true,
"docs":[{
  "patent_number":[10000001],
  "patent_date":["2018-06-19T00:00:00Z"],
  "patent_title":["Injection molding machine and mold thickness control method"],
  "id":"32c5c531-06fb-4449-abd7-91537dab1b83",
  "_version_":1772645225243607040
},{
  "patent_number":[10000001],
  "patent_date":["2018-06-19T00:00:00Z"],
  "patent_title":["Injection molding machine and mold thickness control method"],
  "id":"2a96abdb-e54a-4817-a7da-4434b6ef4b1d",
  "_version_":1772645225369436160
},{
  "patent_number":[10000002],
  "patent_date":["2018-06-19T00:00:00Z"],
  "patent_title":["Method for manufacturing polymer film and co-extruded film"],
  "id":"b7da25e1-7a6d-4db7-8a6f-a30b82b573c0",
  "_version_":1772645225272967168
},{
  "patent_number":[10000002],
  "patent_date":["2018-06-19T00:00:00Z"],
  "patent_title":["Method for manufacturing polymer film and co-extruded film"],
  "id":"150e4d4b-d6fa-4d9d-9656-27f508dd4af7",
  "_version_":1772645225397747712
},{
  "patent_number":[10000003],
  "patent_date":["2018-06-19T00:00:00Z"],
  "patent_title":["Method for producing a container from a thermoplastic"],
  "id":"a99c22a8-a179-4e98-a23b-abf3488e251d",
  "_version_":1772645225306521600
}]
}
}

```

### Fetched data in elastic-search (only few shown here ) 
```
ambujsingh@HAT-ADMINs-MacBook-Pro ~ % curl -X GET "http://localhost:9200/new_patents_v1/_search?q=*" | jq .

  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100  2155  100  2155    0     0  59104      0 --:--:-- --:--:-- --:--:-- 69516
{
  "took": 20,
  "timed_out": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": {
      "value": 978,
      "relation": "eq"
    },
    "max_score": 1,
    "hits": [
      {
        "_index": "new_patents_v1",
        "_type": "_doc",
        "_id": "10000943",
        "_score": 1,
        "_source": {
          "patent_number": "10000943",
          "patent_date": "2018-06-19",
          "patent_title": "Beautification and privacy fence panel system and uses thereof"
        }
      },
           {
        "_index": "new_patents_v1",
        "_type": "_doc",
        "_id": "10000947",
        "_score": 1,
        "_source": {
          "patent_number": "10000947",
          "patent_date": "2018-06-19",
          "patent_title": "Auto-lock system for door window frame lateral retention"
        }
      }

ambujsingh@HAT-ADMINs-MacBook-Pro ~ % 
```
